<!DOCTYPE html>
<html lang="zh">

<head>
	<meta charset="UTF-8">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
	<meta name="viewport" content="width=device-width,initial-scale=1.0,user-scalable=no" />
	<meta name="apple-mobile-web-app-capable" content="yes" />
	<meta name="apple-mobile-web-app-status-bar-style" content="black" />
	<meta name="renderer" content="webkit" />
	<meta name="screen-orientation" content="portrait" />
	<meta name="x5-orientation" content="portrait" />
	<title>AIMV Laboratory|Artificial Intelligence and Machine Vision</title>
	<link rel="stylesheet" type="text/css" href="css/style.css" />
</head>

<body>
	<div class="top clearfix">
		<div class="header-navbar">
			<a href="index.html" target="_blank" class="logo fl"><img src="img/logo.png" /></a>
			<ul class="navbar clearfix fl">
				<li><a href="index.html" style="font-size: 16px;">Home</a></li>
				<li><a href="supervisor.html" style="font-size: 16px;">Team</a></li>
				<li><a href="direction.html" style="font-size: 16px;">Research</a></li>
				<li><a href="publication.html" style="font-size: 16px;">Publication</a></li>
				<li><a href="resource.html" style="font-size: 16px;">Resource</a></li>
				<li><a href="news.html" style="font-size: 16px;">News</a></li>
				<li><a href="graduates.html" style="font-size: 16px;">Alumni</a></li>
				<li><a href="index_cn.html" style="font-size: 16px;">Chinese</a></li>
			</ul>
		</div>
	</div>
	<div class="banner">
		<ul class="bannerfix slides clearfix">
			<li>
				<img class="bimg" src="img/slide/10.jpg" />
			</li>
			<li>
				<img class="bimg" src="img/slide/2.jpg" />
			</li>
			<li>
				<img class="bimg" src="img/slide/11.jpg" />
			</li>
			<li>
				<img class="bimg" src="img/slide/4.jpg" />
			</li>
			<li>
				<img class="bimg" src="img/slide/5.jpg" />
			</li>
			<li>
				<img class="bimg" src="img/slide/6.jpg" />
			</li>
			<li>
				<img class="bimg" src="img/slide/7.jpg" />
			</li>
			<li>
				<img class="bimg" src="img/slide/8.jpg" />
			</li>
			<li>
				<img class="bimg" src="img/slide/9.jpg" />
			</li>
			<li>
				<img class="bimg" src="img/slide/20.jpg" />
			</li>
			<li>
				<img class="bimg" src="img/slide/21.jpg" />
			</li>
			<li>
				<img class="bimg" src="img/slide/22.jpg" />
			</li>
			<li>
				<img class="bimg" src="img/slide/23.jpg" />
			</li>
		</ul>
		<div class="bamnline"><span></span></div>

	</div>

	<div class="index-sec2">
		<div class="w1240">
			<div class="w1240 de-box">
				<div class="clearfix ">
					<div class="detail-left">
						<div class="stace">
							<h1>Lab Introduction</h1>
							<div class="cont">
								<p style='text-align: justify'>
									The artificial intelligence and machine vision (AIMV) Lab was established in 2017 to
									conduct AI-related research and applications, which mainly focus on computer vision,
									image/video processing, pattern recognition, robotics, computational intelligence,
									and machine learning. Currently, AIMV Lab comprises 6 faculties, 2 post doctors, 12
									PhD candidates, and more than 50 Master students, which is an interdisciplinary
									research team including researchers from mathematics, computer engineering,
									electronic and control engineering, and mechanical engineering.
									We have undertaken more than 10 projects sponsored by the National Natural Science
									Foundation of China, the Department of Science & Technology, Hubei Province, the
									Department of Education, Hubei Province, and other industrial projects. We have
									published more than 400 papers in premium journals and conferences such as IEEE
									Trans. Image Processing, IEEE Trans. Circuits and Systems for Video Technology, IEEE
									Trans. Cybernetics, IEEE Conference on Computer Vision and Pattern Recognition, IEEE
									International Conference on Computer Vision. More than 20 China patents have been
									granted.
								</p>
							</div>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>

	<div class="index-sec3">
		<div class="tit">
			<h2>Latest Achievements</h2>
		</div>
		<div class="w1240">
			<div class="index-content">
				<div class="bd">
					<ul class="picList" style="list-style:none;">
						<li>
							<a href="https://ieeexplore.ieee.org/document/10125062">
								<figure>
									<img src="img/news/zhang.png" />
								</figure>
								<figcaption>
									<p class="p1">Self-Calibrating Gaze Estimation with Optical Axes Projection for Head-Mounted Eye Tracking</p>
									<p class="p2">Gaze estimation suffers from burdensome personal calibration or complex all-device calibration.
										Self calibrating methods can meet this challenge but depend on scenes and sacrifice accuracy. 
										We propose a flexible and accurate gaze estimation approach calibrated implicitly with potential gaze patterns.
										By constructing an optical axis projection (OAP) plane and a visual axis projection (VAP) plane simultaneously
										 ...</p>
								</figcaption>
							</a>
						</li>
						
						<li>
							<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10081014">
								<figure>
									<img src="img/gaozheng.jpg" />
								</figure>
								<figcaption>
									<p class="p1">A Hybrid Method for Implicit Intention Inference
										Based on Punished-Weighted Na√Øve Bayes</p>
									<p class="p2">Gaze-based implicit intention inference
										provides a new human-robot interaction for people
										with disabilities to accomplish activities of daily living
										independently. Existing gaze-based intention inference is
										mainly implemented by the data-driven method without
										prior object information in intention expression, which
										yields low inference accuracy. Aiming to improve the
										inference accuracy, we propose a gaze-based hybrid
										method by integrating model-driven and data-driven
										intention inference tailored to disability ...</p>
								</figcaption>
							</a>
						</li>
						<li>
							<a href="https://arxiv.org/pdf/2209.05913.pdf">
								<figure>
									<img src="img/Dual Scale Single.jpg" />
								</figure>
								<figcaption>
									<p class="p1">Dual-Scale Single Image Dehazing via Neural Augmentation</p>
									<p class="p2">Model-based single image dehazing algorithms restore haze-free images
										with sharp edges and rich details for real-world
										hazy images at the expense of low PSNR and SSIM values for synthetic hazy
										images. Data-driven ones restore haze-free images
										with high PSNR and SSIM values for synthetic hazy images but with low contrast,
										and even some remaining haze for realworld
										hazy images. In this paper, a novel single image dehazing algorithm is
										introduced by combining model-based and datadriven
										...</p>
								</figcaption>
							</a>
						</li>
						<li>
							<a href="https://doi.org/10.1016/j.patcog.2022.108900">
								<figure>
									<img src="img/AWGIF.png" />
								</figure>
								<figcaption>
									<p class="p1">Adaptive weighted guided image filtering for depth enhancement in
										shape-from-focus</p>
									<p class="p2">Existing shape from focus (SFF) techniques cannot preserve depth edges
										and fine structural details
										from a sequence of multi-focus images. Moreover, noise in the sequence of
										multi-focus images affects
										the accuracy of the depth map. In this paper, a novel depth enhancement
										algorithm for the SFF based
										on an adaptive weighted guided image filtering (AWGIF) is proposed to address
										the above issues.The AWGIF is applied to decompose an initial depth map which is
										estimated by the traditional SFF
										into a base layer...</p>
								</figcaption>
							</a>
						</li>

						<li>
							<a href="https://www.mdpi.com/2071-1050/14/22/15329">
								<figure>
									<img src="img/sustainability.jpg" />
								</figure>
								<figcaption>
									<p class="p1">Water Column Detection Method at Impact Point Based on
										Improved YOLOv4 Algorithm</p>
									<p class="p2">For a long time, the water column at the impact point of a naval gun
										firing at the sea
										has mainly depended on manual detection methods for locating, which has problems
										such as low
										accuracy, subjectivity and inefficiency. In order to solve the above problems,
										this paper proposes
										a water column detection method based on an improved you-only-look-once version
										4 (YOLOv4)
										algorithm. Firstly, the method detects the sea antenna through the Hoffman line
										detection method...</p>
								</figcaption>
							</a>
						</li>

						<li>
							<a
								href="https://oar.a-star.edu.sg/storage/d/d62ee21rxr/image-noise-level-estimation-via-kurtosis-test.pdf">
								<figure>
									<img src="img/noise test.png" />
								</figure>
								<figcaption>
									<p class="p1">Image noise level estimation via kurtosis test</p>
									<p class="p2">Noise level estimation is a long-standing problem in image processing.
										The challenge arises from the fact 7 that the estimation can be easily affected
										by
										texture information. In this paper, a new noise level estimation method 8 via
										kurtosis
										test is proposed, which is a normalized fourth-order moment. The proposed method
										consists
										of two stages: 9 the first one is to determine the image patches with normality
										by using
										the kurtosis test, the noise level is then estimated from these selected normal
										patches in the second stage...</p>
								</figcaption>
							</a>
						</li>


						<li>
							<a href="https://ieeexplore.ieee.org/abstract/document/9560022/">
								<figure>
									<img src="img/pupil.png" />
								</figure>
								<figcaption>
									<p class="p1">Pupil-Contour-Based Gaze Estimation With Real Pupil Axes for
										Head-Mounted Eye Tracking</p>
									<p class="p2">Accurate gaze estimation that frees from glints and the slippage
										problem is challenging.
										Pupil-contour-based gaze estimation methods can meet this challenge, except that
										the gaze accuracy is
										low due to neglecting the pupil‚Äôs corneal refraction This article proposes a
										refraction-aware gaze estimation
										approach using the real pupil axis, which is calculated from the virtual pupil
										image based on the derived
										function between the real pupil ...</p>
								</figcaption>
							</a>
						</li>
						<li>
							<a
								href="http://openaccess.thecvf.com/content/CVPR2021/html/Xu_Adaptive_Rank_Estimate_in_Robust_Principal_Component_Analysis_CVPR_2021_paper.html">
								<figure>
									<img src="img/ARE.png" />
								</figure>
								<figcaption>
									<p class="p1">Adaptive rank estimate in robust principal component analysis</p>
							
									
									<p class="p2">Robust principal component analysis (RPCA) and its variants have
										gained wide applications
										in computer vision. However, these methods either involve manual adjustment of
										some
										parameters, or require the rank of a low-rank matrix to be known a prior. In
										this paper, an
										adaptive rank estimate based RPCA (ARE-RPCA) is proposed, which adaptively
										assigns
										weights on different singular values via rank estimation. More specifically, we
										study the
										characteristics of the low-rank matrix, and develop an improved Gerschgorin disk
										theorem to ...</p>
								</figcaption>
							</a>
						</li>
						<!-- <li>
						<a href="https://openaccess.thecvf.com/content/ICCV2021W/RSLCV/html/Xu_Double-Weighted_Low-Rank_Matrix_Recovery_Based_on_Rank_Estimation_ICCVW_2021_paper.html">
							<figure>
								<img src="img/double.jpg"/>
							</figure>
							<figcaption>
								<p class="p1">Double-weighted low-rank matrix recovery based on rank estimation</p>
								<p class="p2">Robust principal component analysis (RPCA) has widely application in computer vision and
									data mining. However, the various RPCA algorithms in practical applications need to know
									the rank of low-rank matrix in advance, or manually adjust parameters. To overcome these
									limitations, an adaptive double-weighted RPCA algorithm is proposed to recover low-rank
									matrix accurately based on the estimated rank of the low-rank matrix and the reweighting
									strategy in this paper. More specifically, the Gerschgorin's disk theorem is introduced to ...</p>
							</figcaption>
						</a>
					</li> -->
						
					</ul>
				</div>
				<div class="hd">
					<span class="prev"></span>
					<span class="next"></span>
				</div>
			</div>
		</div>
	</div>

	<div class="footer">
		<div class="w1240">
			<div class="div1 clearfix">
				<ul class="clearfix fr">
					<li><a href="index.html">Home</a></li>
					<li><a href="supervisor.html">Team</a></li>
					<li><a href="direction.html">Research</a></li>
					<li><a href="publication.html">Publication</a></li>
					<li><a href="resource.html">Resource</a></li>
					<li><a href="news.html">News</a></li>
					<li><a href="graduates.html">Alumni</a></li>
					<li><a href="index_cn.html">Chinese</a></li>
				</ul>
			</div>
			<div class="div2 clearfix">
				<div class="left fl">
					<p class="p2">
						Useful link:
						<a href="https://www.csail.mit.edu/" class="num">https://www.csail.mit.edu/</a>
						<a href="https://www.ri.cmu.edu/" class="num">https://www.ri.cmu.edu/</a>
						<a href="https://ai.stanford.edu/" class="num">https://ai.stanford.edu/</a>
						<a href="https://www.crcv.ucf.edu/" class="num">https://www.crcv.ucf.edu/</a>
					</p>
					<p class="p1">
						School of Information Science and Engineering, Wuhan University of Science and Technology
						<br />
						Address: 947 Heping Avenue, Qingshan District, Wuhan 430081, Hubei, P.R. China
						<br />
						Copyright¬©2022 Artificial Intelligence and Machine Vision Lab<br />

						<a href="https://beian.miit.gov.cn/" target="_blank"  class="num">ÈÑÇICPÂ§á2022008689Âè∑-1</a>
					
						<!-- <a href="https://beian.miit.gov.cn" class="num">EICP No.2022008689-1</a> -->
					</p>
<!-- 					<p class="p1">User sessions:<span style="color:white;" id="busuanzi_value_site_pv"></span></p> -->
					<p class="p1"style="font-size:20px;"> User sessions: <img src="https://profile-counter.glitch.me/aimv-lab/count.svg" /> </p>
					
				</div>

				<div class="right fr">
					<p class="p3">E-mail</p>
					<p class="p4"><a href="mailto:shiqian.wu@wust.edu.cn">shiqian.wu@wust.edu.cn</a></p>
				</div>
			</div>
		</div>
	</div>




	<script src="js/jquery.js"></script>
	<script src="js/SuperSlide.js"></script>
	<script src="js/plugin.js"></script>
	<script src="js/banner.js"></script>
	<script src="js/index.js"></script>
	<script src="js/more.js"></script>
<!-- 	<script async src="/suanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> <!--ÁªüËÆ°ËÆøÈóÆÈáèÁöÑjs--> -->
</body>

</html>
